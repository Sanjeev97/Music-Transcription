{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4uxB7B3ErmfVNdgjA7lif"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kKpPa8f4rUaf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import librosa\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "class GuitarSetDataset(Dataset):\n",
        "    def __init__(self, data_dir, split='train', sample_rate=44100, hop_length=512, segment_size=5):\n",
        "        \"\"\"\n",
        "        Dataset for GuitarSet (https://guitarset.weebly.com/)\n",
        "\n",
        "        Args:\n",
        "            data_dir: Root directory of GuitarSet\n",
        "            split: 'train' or 'test'\n",
        "            sample_rate: Audio sample rate\n",
        "            hop_length: Hop length for STFT\n",
        "            segment_size: Length of audio segments in seconds\n",
        "        \"\"\"\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.sample_rate = sample_rate\n",
        "        self.hop_length = hop_length\n",
        "        self.segment_size = segment_size\n",
        "\n",
        "        # All player IDs in GuitarSet\n",
        "        all_players = ['00', '01', '02', '03', '04', '05']\n",
        "\n",
        "        # Split into train and test (adjust as needed)\n",
        "        if split == 'train':\n",
        "            self.players = all_players[:4]  # Players 00-03 for training\n",
        "        else:\n",
        "            self.players = all_players[4:]  # Players 04-05 for testing\n",
        "\n",
        "        self.file_pairs = self._get_file_pairs()\n",
        "\n",
        "    def _get_file_pairs(self):\n",
        "        \"\"\"Get all matching audio and annotation file pairs\"\"\"\n",
        "        file_pairs = []\n",
        "\n",
        "        # GuitarSet structure: {player}/{style}/{piece}\n",
        "        for player in self.players:\n",
        "            player_dir = self.data_dir / f\"guitarist_{player}\"\n",
        "\n",
        "            # Find all audio files\n",
        "            audio_files = list(player_dir.glob(\"**/*audio_hex_cln.wav\"))\n",
        "\n",
        "            for audio_file in audio_files:\n",
        "                # Find corresponding annotation file\n",
        "                jams_file = audio_file.with_name(audio_file.stem.replace('audio_hex_cln', 'jams'))\n",
        "                jams_file = jams_file.with_suffix('.jams')\n",
        "\n",
        "                if jams_file.exists():\n",
        "                    file_pairs.append((audio_file, jams_file))\n",
        "\n",
        "        return file_pairs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_pairs)\n",
        "\n",
        "    def _parse_jams(self, jams_path):\n",
        "        \"\"\"Parse JAMS file to extract guitar note events\"\"\"\n",
        "        import jams\n",
        "\n",
        "        # Load JAMS file\n",
        "        jam = jams.load(jams_path)\n",
        "\n",
        "        # Get note annotations\n",
        "        notes_data = []\n",
        "        for annotation in jam.annotations:\n",
        "            if annotation.namespace == 'note_midi':\n",
        "                for note in annotation.data:\n",
        "                    notes_data.append({\n",
        "                        'time': float(note.time),\n",
        "                        'duration': float(note.duration),\n",
        "                        'value': int(note.value),\n",
        "                        'string': int(note.value) % 6,  # Extract string information\n",
        "                        'fret': (int(note.value) - 40) // 6  # Approximate fret position\n",
        "                    })\n",
        "\n",
        "        return notes_data\n",
        "\n",
        "    def _notes_to_piano_roll(self, notes, duration):\n",
        "        \"\"\"Convert note events to a piano roll format\"\"\"\n",
        "        # Create piano roll array\n",
        "        n_frames = int(duration * self.sample_rate / self.hop_length)\n",
        "        piano_roll = np.zeros((128, n_frames), dtype=np.float32)\n",
        "\n",
        "        for note in notes:\n",
        "            # Convert time to frame\n",
        "            start_frame = int(note['time'] * self.sample_rate / self.hop_length)\n",
        "            end_frame = int((note['time'] + note['duration']) * self.sample_rate / self.hop_length)\n",
        "\n",
        "            # Limit to array bounds\n",
        "            end_frame = min(end_frame, n_frames)\n",
        "\n",
        "            if start_frame < n_frames and start_frame < end_frame:\n",
        "                piano_roll[note['value'], start_frame:end_frame] = 1.0\n",
        "\n",
        "        return piano_roll\n",
        "\n",
        "    def _create_guitar_roll(self, notes, duration):\n",
        "        \"\"\"Create a guitar-specific representation (string+fret)\"\"\"\n",
        "        n_frames = int(duration * self.sample_rate / self.hop_length)\n",
        "\n",
        "        # 6 strings × max fret position (20)\n",
        "        guitar_roll = np.zeros((6, 21, n_frames), dtype=np.float32)\n",
        "\n",
        "        for note in notes:\n",
        "            # Convert time to frame\n",
        "            start_frame = int(note['time'] * self.sample_rate / self.hop_length)\n",
        "            end_frame = int((note['time'] + note['duration']) * self.sample_rate / self.hop_length)\n",
        "\n",
        "            # Limit to array bounds\n",
        "            end_frame = min(end_frame, n_frames)\n",
        "\n",
        "            # Get string and fret\n",
        "            string = note.get('string', 0)\n",
        "            fret = min(note.get('fret', 0), 20)  # Limit to 20 frets\n",
        "\n",
        "            # String-fret combination\n",
        "            if 0 <= string < 6 and 0 <= fret <= 20 and start_frame < n_frames and start_frame < end_frame:\n",
        "                guitar_roll[string, fret, start_frame:end_frame] = 1.0\n",
        "\n",
        "        return guitar_roll\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_path, jams_path = self.file_pairs[idx]\n",
        "\n",
        "        # Load audio\n",
        "        audio, sr = librosa.load(audio_path, sr=self.sample_rate, mono=True)\n",
        "        duration = len(audio) / sr\n",
        "\n",
        "        # Extract features\n",
        "        mel_spec = librosa.feature.melspectrogram(\n",
        "            y=audio,\n",
        "            sr=sr,\n",
        "            n_mels=128,\n",
        "            hop_length=self.hop_length\n",
        "        )\n",
        "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "\n",
        "        # Normalize\n",
        "        mel_spec_db = (mel_spec_db - mel_spec_db.mean()) / (mel_spec_db.std() + 1e-8)\n",
        "\n",
        "        # Parse annotations\n",
        "        notes = self._parse_jams(jams_path)\n",
        "\n",
        "        # Create target outputs\n",
        "        piano_roll = self._notes_to_piano_roll(notes, duration)\n",
        "        guitar_roll = self._create_guitar_roll(notes, duration)\n",
        "\n",
        "        # Convert to torch tensors\n",
        "        mel_spec_db = torch.FloatTensor(mel_spec_db)\n",
        "        piano_roll = torch.FloatTensor(piano_roll)\n",
        "        guitar_roll = torch.FloatTensor(guitar_roll)\n",
        "\n",
        "        return {\n",
        "            'input': mel_spec_db,\n",
        "            'piano_roll': piano_roll,\n",
        "            'guitar_roll': guitar_roll,\n",
        "            'audio_path': str(audio_path)\n",
        "        }\n",
        "\n",
        "class GuitarTranscriber(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GuitarTranscriber, self).__init__()\n",
        "\n",
        "        # Feature extraction layers\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "        # Bidirectional LSTM for temporal modeling\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=256 * 8,  # Depends on input size and pooling\n",
        "            hidden_size=512,\n",
        "            num_layers=2,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=0.3\n",
        "        )\n",
        "\n",
        "        # Output layers for note detection\n",
        "        self.note_classifier = nn.Sequential(\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 128)  # 128 MIDI notes\n",
        "        )\n",
        "\n",
        "        # Output layers for guitar-specific representation\n",
        "        self.guitar_classifier = nn.Sequential(\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 6 * 21)  # 6 strings × 21 frets (0-20)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, freq_bins, time_frames = x.shape\n",
        "\n",
        "        # Add channel dimension\n",
        "        x = x.unsqueeze(1)\n",
        "\n",
        "        # Feature extraction\n",
        "        x = self.conv_layers(x)\n",
        "\n",
        "        # Reshape for LSTM: [batch, channels, freq, time] -> [batch, time, channels*freq]\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        x = x.reshape(batch_size, x.shape[1], -1)\n",
        "\n",
        "        # LSTM for temporal modeling\n",
        "        x, _ = self.lstm(x)\n",
        "\n",
        "        # Note prediction\n",
        "        note_output = self.note_classifier(x)\n",
        "\n",
        "        # Guitar-specific prediction\n",
        "        guitar_output = self.guitar_classifier(x)\n",
        "        guitar_output = guitar_output.reshape(batch_size, time_frames, 6, 21)\n",
        "\n",
        "        return note_output, guitar_output\n",
        "\n",
        "def train_guitar_transcriber(data_dir, num_epochs=50, batch_size=16, learning_rate=0.001):\n",
        "    # Create datasets and dataloaders\n",
        "    train_dataset = GuitarSetDataset(data_dir, split='train')\n",
        "    test_dataset = GuitarSetDataset(data_dir, split='test')\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "    # Model, loss function and optimizer\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = GuitarTranscriber().to(device)\n",
        "\n",
        "    criterion_notes = nn.BCEWithLogitsLoss()\n",
        "    criterion_guitar = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    history = {'train_loss': [], 'val_loss': []}\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} - Training'):\n",
        "            inputs = batch['input'].to(device)\n",
        "            piano_roll_targets = batch['piano_roll'].to(device)\n",
        "            guitar_roll_targets = batch['guitar_roll'].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            note_outputs, guitar_outputs = model(inputs)\n",
        "\n",
        "            # Calculate losses\n",
        "            note_loss = criterion_notes(note_outputs, piano_roll_targets)\n",
        "            guitar_loss = criterion_guitar(guitar_outputs, guitar_roll_targets)\n",
        "\n",
        "            # Combined loss (weighted as needed)\n",
        "            loss = note_loss + 0.5 * guitar_loss\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(test_loader, desc=f'Epoch {epoch+1}/{num_epochs} - Validation'):\n",
        "                inputs = batch['input'].to(device)\n",
        "                piano_roll_targets = batch['piano_roll'].to(device)\n",
        "                guitar_roll_targets = batch['guitar_roll'].to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                note_outputs, guitar_outputs = model(inputs)\n",
        "\n",
        "                # Calculate losses\n",
        "                note_loss = criterion_notes(note_outputs, piano_roll_targets)\n",
        "                guitar_loss = criterion_guitar(guitar_outputs, guitar_roll_targets)\n",
        "\n",
        "                # Combined loss\n",
        "                loss = note_loss + 0.5 * guitar_loss\n",
        "\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(test_loader)\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "        # Print progress\n",
        "        print(f'Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "        # Save history\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "        history['val_loss'].append(avg_val_loss)\n",
        "\n",
        "        # Save best model\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save(model.state_dict(), 'guitar_transcriber_best.pth')\n",
        "\n",
        "    # Save final model\n",
        "    torch.save(model.state_dict(), 'guitar_transcriber_final.pth')\n",
        "\n",
        "    # Plot training history\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(history['train_loss'], label='Train Loss')\n",
        "    plt.plot(history['val_loss'], label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Training History')\n",
        "    plt.savefig('training_history.png')\n",
        "\n",
        "    return model, history\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader, device):\n",
        "    \"\"\"Evaluate the model performance\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc='Evaluating'):\n",
        "            inputs = batch['input'].to(device)\n",
        "            piano_roll_targets = batch['piano_roll'].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            note_outputs, _ = model(inputs)\n",
        "\n",
        "            # Apply sigmoid to get probabilities\n",
        "            note_probs = torch.sigmoid(note_outputs)\n",
        "\n",
        "            # Convert to binary predictions using threshold\n",
        "            note_preds = (note_probs > 0.5).float()\n",
        "\n",
        "            # Collect predictions and targets for metrics\n",
        "            all_preds.append(note_preds.cpu().numpy())\n",
        "            all_targets.append(piano_roll_targets.cpu().numpy())\n",
        "\n",
        "    # Concatenate all batches\n",
        "    all_preds = np.concatenate(all_preds, axis=0)\n",
        "    all_targets = np.concatenate(all_targets, axis=0)\n",
        "\n",
        "    # Calculate metrics\n",
        "    precision, recall, f1 = calculate_frame_metrics(all_targets, all_preds)\n",
        "\n",
        "    print(f\"Frame-level Metrics:\")\n",
        "    print(f\"  Precision: {precision:.4f}\")\n",
        "    print(f\"  Recall: {recall:.4f}\")\n",
        "    print(f\"  F1 Score: {f1:.4f}\")\n",
        "\n",
        "    return {'precision': precision, 'recall': recall, 'f1': f1}\n",
        "\n",
        "def calculate_frame_metrics(targets, predictions):\n",
        "    \"\"\"Calculate frame-level precision, recall, and F1 score\"\"\"\n",
        "    # Reshape to [frames, notes]\n",
        "    targets_flat = targets.reshape(-1, targets.shape[-1])\n",
        "    preds_flat = predictions.reshape(-1, predictions.shape[-1])\n",
        "\n",
        "    # Calculate true positives, false positives, false negatives\n",
        "    true_positives = np.sum(np.logical_and(targets_flat == 1, preds_flat == 1))\n",
        "    false_positives = np.sum(np.logical_and(targets_flat == 0, preds_flat == 1))\n",
        "    false_negatives = np.sum(np.logical_and(targets_flat == 1, preds_flat == 0))\n",
        "\n",
        "    # Calculate metrics\n",
        "    precision = true_positives / (true_positives + false_positives + 1e-8)\n",
        "    recall = true_positives / (true_positives + false_negatives + 1e-8)\n",
        "    f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
        "\n",
        "    return precision, recall, f1\n",
        "\n",
        "def transcribe_guitar_audio(model, audio_path, output_dir=\"transcriptions\"):\n",
        "    \"\"\"Transcribe a guitar audio file using the trained model\"\"\"\n",
        "    device = next(model.parameters()).device\n",
        "    model.eval()\n",
        "\n",
        "    # Create output directory\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    file_name = os.path.splitext(os.path.basename(audio_path))[0]\n",
        "\n",
        "    # Load and preprocess audio\n",
        "    y, sr = librosa.load(audio_path, sr=44100, mono=True)\n",
        "\n",
        "    # Extract features\n",
        "    mel_spec = librosa.feature.melspectrogram(\n",
        "        y=y,\n",
        "        sr=sr,\n",
        "        n_mels=128,\n",
        "        hop_length=512\n",
        "    )\n",
        "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "\n",
        "    # Normalize\n",
        "    mel_spec_db = (mel_spec_db - mel_spec_db.mean()) / (mel_spec_db.std() + 1e-8)\n",
        "\n",
        "    # Convert to torch tensor\n",
        "    mel_spec_db = torch.FloatTensor(mel_spec_db).unsqueeze(0).to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    with torch.no_grad():\n",
        "        note_outputs, guitar_outputs = model(mel_spec_db)\n",
        "\n",
        "        # Apply sigmoid to get probabilities\n",
        "        note_probs = torch.sigmoid(note_outputs)\n",
        "        guitar_probs = torch.sigmoid(guitar_outputs)\n",
        "\n",
        "        # Convert to binary predictions using threshold\n",
        "        note_preds = (note_probs > 0.5).float()\n",
        "        guitar_preds = (guitar_probs > 0.5).float()\n",
        "\n",
        "    # Convert predictions to numpy\n",
        "    note_preds = note_preds.squeeze().cpu().numpy()\n",
        "    guitar_preds = guitar_preds.squeeze().cpu().numpy()\n",
        "\n",
        "    # Convert frame-level predictions to note events\n",
        "    midi_data = frames_to_midi(note_preds, sr=sr, hop_length=512)\n",
        "\n",
        "    # Add guitar-specific information\n",
        "    add_guitar_information(midi_data, guitar_preds, sr=sr, hop_length=512)\n",
        "\n",
        "    # Save MIDI file\n",
        "    midi_output_path = os.path.join(output_dir, f\"{file_name}.mid\")\n",
        "    midi_data.write(midi_output_path)\n",
        "\n",
        "    # Create visualization\n",
        "    visualization_path = os.path.join(output_dir, f\"{file_name}_transcription.png\")\n",
        "    create_transcription_visualization(\n",
        "        mel_spec_db.squeeze().cpu().numpy(),\n",
        "        note_preds,\n",
        "        guitar_preds,\n",
        "        visualization_path,\n",
        "        sr=sr,\n",
        "        hop_length=512\n",
        "    )\n",
        "\n",
        "    return midi_output_path\n",
        "\n",
        "def frames_to_midi(note_frames, sr=44100, hop_length=512, velocity=100):\n",
        "    \"\"\"Convert frame-wise predictions to MIDI notes\"\"\"\n",
        "    midi_data = pretty_midi.PrettyMIDI()\n",
        "\n",
        "    # Add acoustic guitar instrument\n",
        "    guitar = pretty_midi.Instrument(program=24)  # 24 = Acoustic Guitar (nylon)\n",
        "\n",
        "    # Helper function to convert frame index to time\n",
        "    def frame_to_time(frame_idx):\n",
        "        return frame_idx * hop_length / sr\n",
        "\n",
        "    # Process each pitch\n",
        "    for pitch in range(note_frames.shape[1]):\n",
        "        # Skip pitches outside guitar range (E2 to E6)\n",
        "        if pitch < 40 or pitch > 88:\n",
        "            continue\n",
        "\n",
        "        # Find continuous segments\n",
        "        frames = note_frames[:, pitch]\n",
        "        begins = np.where(np.diff(np.concatenate(([0], frames))) > 0)[0]\n",
        "        ends = np.where(np.diff(np.concatenate((frames, [0]))) < 0)[0]\n",
        "\n",
        "        # Create notes for each segment\n",
        "        for start, end in zip(begins, ends):\n",
        "            # Only create notes with minimum duration\n",
        "            if end - start >= 2:  # At least 2 frames\n",
        "                note = pretty_midi.Note(\n",
        "                    velocity=velocity,\n",
        "                    pitch=pitch,\n",
        "                    start=frame_to_time(start),\n",
        "                    end=frame_to_time(end)\n",
        "                )\n",
        "                guitar.notes.append(note)\n",
        "\n",
        "    midi_data.instruments.append(guitar)\n",
        "    return midi_data\n",
        "\n",
        "def add_guitar_information(midi_data, guitar_preds, sr=44100, hop_length=512):\n",
        "    \"\"\"Add guitar-specific information (string/fret) to MIDI file\"\"\"\n",
        "    if len(midi_data.instruments) == 0:\n",
        "        return\n",
        "\n",
        "    guitar = midi_data.instruments[0]\n",
        "\n",
        "    # Helper function to convert frame index to time\n",
        "    def frame_to_time(frame_idx):\n",
        "        return frame_idx * hop_length / sr\n",
        "\n",
        "    # Add string/fret information as text annotations\n",
        "    for note in guitar.notes:\n",
        "        start_frame = int(note.start * sr / hop_length)\n",
        "        end_frame = int(note.end * sr / hop_length)\n",
        "\n",
        "        # Limit to valid frame indices\n",
        "        start_frame = max(0, min(start_frame, guitar_preds.shape[2] - 1))\n",
        "        end_frame = max(0, min(end_frame, guitar_preds.shape[2] - 1))\n",
        "\n",
        "        # Skip if no valid frames\n",
        "        if start_frame >= end_frame:\n",
        "            continue\n",
        "\n",
        "        # Find most likely string/fret combination for this note\n",
        "        max_prob = 0\n",
        "        best_string = 0\n",
        "        best_fret = 0\n",
        "\n",
        "        for string in range(6):\n",
        "            for fret in range(21):\n",
        "                # Average probability over the note duration\n",
        "                avg_prob = np.mean(guitar_preds[string, fret, start_frame:end_frame])\n",
        "\n",
        "                if avg_prob > max_prob:\n",
        "                    max_prob = avg_prob\n",
        "                    best_string = string\n",
        "                    best_fret = fret\n",
        "\n",
        "        # Store information in the note (using pretty_midi's internal storage)\n",
        "        # This would be accessible in the MIDI file's metadata\n",
        "        # For visualization, you might want to store this separately\n",
        "        note.string = best_string\n",
        "        note.fret = best_fret\n",
        "\n",
        "def create_transcription_visualization(mel_spec, note_preds, guitar_preds, output_path, sr=44100, hop_length=512):\n",
        "    \"\"\"Create visualization of the transcription results\"\"\"\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Plot mel spectrogram\n",
        "    plt.subplot(3, 1, 1)\n",
        "    plt.title(\"Mel Spectrogram\")\n",
        "    librosa.display.specshow(\n",
        "        mel_spec,\n",
        "        sr=sr,\n",
        "        hop_length=hop_length,\n",
        "        x_axis='time',\n",
        "        y_axis='mel'\n",
        "    )\n",
        "    plt.colorbar(format='%+2.0f dB')\n",
        "\n",
        "    # Plot transcribed notes\n",
        "    plt.subplot(3, 1, 2)\n",
        "    plt.title(\"Transcribed Notes\")\n",
        "    librosa.display.specshow(\n",
        "        note_preds.T,\n",
        "        sr=sr,\n",
        "        hop_length=hop_length,\n",
        "        x_axis='time',\n",
        "        y_axis='midi'\n",
        "    )\n",
        "    plt.colorbar()\n",
        "\n",
        "    # Plot guitar-specific information (combine strings and frets)\n",
        "    plt.subplot(3, 1, 3)\n",
        "    plt.title(\"Guitar String/Fret Activations\")\n",
        "    # Sum over frets to get string activations for visualization\n",
        "    string_activations = np.sum(guitar_preds, axis=1)\n",
        "    librosa.display.specshow(\n",
        "        string_activations,\n",
        "        sr=sr,\n",
        "        hop_length=hop_length,\n",
        "        x_axis='time'\n",
        "    )\n",
        "    plt.yticks(np.arange(6), ['E', 'A', 'D', 'G', 'B', 'e'])\n",
        "    plt.ylabel('String')\n",
        "    plt.colorbar()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(output_path)\n",
        "    plt.close()\n",
        "\n",
        "def download_guitarset():\n",
        "    \"\"\"Helper function to download GuitarSet dataset from the correct location\"\"\"\n",
        "    import urllib.request\n",
        "    import zipfile\n",
        "    import os\n",
        "\n",
        "    # Current URLs for GuitarSet (as of 2024)\n",
        "    urls = [\n",
        "        'https://zenodo.org/records/3371780/files/audio_hex-pickup_original.zip?download=1',\n",
        "        'https://zenodo.org/records/3371780/files/annotation.zip?download=1'\n",
        "    ]\n",
        "\n",
        "    # Create directory\n",
        "    os.makedirs('guitarset', exist_ok=True)\n",
        "\n",
        "    # Download and extract\n",
        "    for url in urls:\n",
        "        file_name = os.path.join('guitarset', os.path.basename(url))\n",
        "        print(f\"Downloading {url}...\")\n",
        "\n",
        "        try:\n",
        "            # Download\n",
        "            urllib.request.urlretrieve(url, file_name)\n",
        "\n",
        "            # Extract\n",
        "            print(f\"Extracting {file_name}...\")\n",
        "            with zipfile.ZipFile(file_name, 'r') as zip_ref:\n",
        "                zip_ref.extractall('guitarset')\n",
        "\n",
        "        except urllib.error.HTTPError as e:\n",
        "            print(f\"Error downloading {url}: {e}\")\n",
        "            print(\"Attempting alternative download method...\")\n",
        "\n",
        "            # Try alternative method or sources\n",
        "            alternative_source = url.replace('zenodo.org/records/', 'zenodo.org/record/')\n",
        "            try:\n",
        "                print(f\"Trying alternative URL: {alternative_source}\")\n",
        "                urllib.request.urlretrieve(alternative_source, file_name)\n",
        "\n",
        "                print(f\"Extracting {file_name}...\")\n",
        "                with zipfile.ZipFile(file_name, 'r') as zip_ref:\n",
        "                    zip_ref.extractall('guitarset')\n",
        "\n",
        "            except Exception as e2:\n",
        "                print(f\"Alternative download also failed: {e2}\")\n",
        "                print(\"\\nSuggested manual download steps:\")\n",
        "                print(\"1. Visit https://guitarset.weebly.com/\")\n",
        "                print(\"2. Download the dataset files manually\")\n",
        "                print(\"3. Extract them to a folder named 'guitarset'\")\n",
        "\n",
        "    # Check if files were downloaded\n",
        "    if os.path.exists('guitarset/annotation') and os.path.exists('guitarset/audio'):\n",
        "        print(\"Download complete.\")\n",
        "    else:\n",
        "        print(\"\\nDownload may have been incomplete. Please try the manual download steps:\")\n",
        "        print(\"1. Visit https://guitarset.weebly.com/\")\n",
        "        print(\"2. Download the dataset files manually\")\n",
        "        print(\"3. Extract them to a folder named 'guitarset'\")\n",
        "\n"
      ],
      "metadata": {
        "id": "M7NLep4rrgxz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Check if CUDA is available\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Option to download dataset\n",
        "    download_dataset = input(\"Download GuitarSet dataset? (y/n): \").lower() == 'y'\n",
        "    if download_dataset:\n",
        "        download_guitarset()\n",
        "\n",
        "    # Set dataset path\n",
        "    data_dir = 'guitarset'\n",
        "\n",
        "    # Training options\n",
        "    train_model = input(\"Train a new model? (y/n): \").lower() == 'y'\n",
        "\n",
        "    if train_model:\n",
        "        model, history = train_guitar_transcriber(\n",
        "            data_dir=data_dir,\n",
        "            num_epochs=30,\n",
        "            batch_size=16,\n",
        "            learning_rate=0.001\n",
        "        )\n",
        "    else:\n",
        "        # Load pre-trained model\n",
        "        model = GuitarTranscriber().to(device)\n",
        "        model_path = input(\"Enter path to pre-trained model (or 'guitar_transcriber_best.pth' for default): \")\n",
        "        if not model_path:\n",
        "            model_path = 'guitar_transcriber_best.pth'\n",
        "\n",
        "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "\n",
        "    # Transcribe a sample file\n",
        "    test_audio = input(\"Enter path to guitar audio file to transcribe: \")\n",
        "    if test_audio:\n",
        "        output_midi = transcribe_guitar_audio(model, test_audio)\n",
        "        print(f\"Transcription saved to {output_midi}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "x7882s7urxYW",
        "outputId": "fdbd713b-2358-447d-a4cc-47d3e6cf2c91"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Download GuitarSet dataset? (y/n): y\n",
            "Downloading https://zenodo.org/records/3371780/files/audio_hex-pickup_original.zip?download=1...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-6312adb2b825>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-6312adb2b825>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdownload_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Download GuitarSet dataset? (y/n): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'y'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdownload_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mdownload_guitarset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Set dataset path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-9dc4dff32856>\u001b[0m in \u001b[0;36mdownload_guitarset\u001b[0;34m()\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0;31m# Download\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0;31m# Extract\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m                 \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    471\u001b[0m                 \u001b[0;31m# clip the read to the \"end of response\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m                 \u001b[0mamt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m                 \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1312\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1314\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1167\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}